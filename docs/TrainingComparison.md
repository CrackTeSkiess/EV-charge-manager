# Hierarchical RL Training Strategies Comparison

## 1. SEQUENTIAL (Recommended)

**Procedure:**
1. Train micro-RL agents on fixed "average" infrastructure (500 episodes)
2. Freeze micro-RL policies
3. Train macro-RL using frozen micro-agents (300 episodes)

**Pros:**
- ✅ Stable training
- ✅ Clear credit assignment
- ✅ Micro-agents learn optimal arbitrage before infrastructure changes
- ✅ 2-3x faster total training time
- ✅ Better final performance

**Cons:**
- ❌ Requires two training phases
- ❌ Micro-agents may not generalize to all infrastructure configs

**Best For:** Production use, reliable results

**Expected Time:** 4-6 hours for 3 stations

---

## 2. SIMULTANEOUS

**Procedure:**
- Train both macro and micro at same time
- Micro updates every hour, macro updates every 10 episodes

**Pros:**
- ✅ Single training phase
- ✅ Micro adapts to specific infrastructure immediately
- ✅ Potential for emergent co-adaptation

**Cons:**
- ❌ Highly unstable
- ❌ Non-stationary problem for both agents
- ❌ Requires careful tuning
- ❌ Often diverges

**Best For:** Research, exploring agent interactions

**Expected Time:** 8-12 hours (with restarts)

---

## 3. CURRICULUM (Most Robust)

**Procedure:**
- Stage 1: Train micro with fixed prices (simple)
- Stage 2: Train macro with simple micro (1-day episodes)
- Stage 3: Fine-tune with variable prices (7-day episodes)

**Pros:**
- ✅ Most robust convergence
- ✅ Best generalization
- ✅ Handles complexity gradually

**Cons:**
- ❌ Longest training time
- ❌ More hyperparameters

**Best For:** Complex scenarios, publication-quality results

**Expected Time:** 12-24 hours

---

## Recommendation

**For most users: Use SEQUENTIAL mode**

```bash
python hierarchical.py --mode sequential --n-agents 3 --save-dir ./my_models
```

Or programmatically:

```python
from ev_charge_manager.rl.hierarchical_trainer import HierarchicalTrainer, TrainingConfig, TrainingMode

config = TrainingConfig(
    mode=TrainingMode.SEQUENTIAL,
    micro_episodes=1000,
    macro_episodes=500,
    output_dir="./my_models",
)
trainer = HierarchicalTrainer(env=env, config=config)
result = trainer.train()
```

---

## Visualising Training Results

Every `hierarchical.py` run automatically generates plots in `{save_dir}/training_plots/` at the end of training. Pass `--no-visualize` to skip this step.

### Regenerate Plots

```bash
# All plots for one run
python visualize.py training --save-dir ./models/hierarchical

# Show interactively instead of (or in addition to) saving
python visualize.py training --save-dir ./models/hierarchical --show
```

### Compare Modes

Use the `compare` subcommand to overlay learning curves and cost breakdowns from multiple runs:

```bash
python visualize.py compare \
    --dirs ./models/sequential ./models/simultaneous ./models/curriculum \
    --output-dir ./plots/comparison
```

This generates `mode_comparison.png` containing:
- **Smoothed learning curves** — one line per mode, same axes
- **Final reward bar chart** — direct comparison of converged performance
- **Horizontal cost breakdown** — grid cost, other costs, and arbitrage profit per mode

### Generated Plot Files

| File | Content |
|------|---------|
| `macro_learning_curves.png` | Macro-RL (PPO) reward curve + losses over training |
| `config_evolution.png` | Best infrastructure config tracked over training episodes |
| `micro_convergence.png` | Per-station micro-RL reward and grid cost convergence |
| `battery_strategy.png` | Learned 24-hour battery charge/discharge pattern |
| `cost_breakdown.png` | Per-station cost decomposition (grid, capex, arbitrage) |
| `curriculum_stages.png` | Reward, cost, and arbitrage improvement across curriculum stages |
| `mode_comparison.png` | Cross-run overlay (generated by `compare` subcommand) |