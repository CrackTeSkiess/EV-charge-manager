\documentclass[journal,12pt,onecolumn]{IEEEtran}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{subfig}
\usepackage{float}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{array}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% -------------------------------------------------------
% AUTO-GENERATED NUMERIC MACROS (do not edit by hand)
% Generated by scripts/generate_paper.py
% -------------------------------------------------------
%%AUTO_MACROS%%
% -------------------------------------------------------

\title{Hierarchical Deep Reinforcement Learning for\\
Energy-Optimal EV Charging Infrastructure\\
on Highway Corridors}

\author{
    \IEEEauthorblockN{Anonymous Author(s)}\\
    \IEEEauthorblockA{Submitted for peer review}
}

\begin{document}

\maketitle

% ============================================================
\begin{abstract}
% ============================================================
The proliferation of battery-electric vehicles on intercity highways demands
intelligent charging infrastructure capable of balancing service quality,
energy cost, and renewable integration.
We present a \textbf{hierarchical deep reinforcement learning (H-DRL)} framework
that jointly optimises macro-level infrastructure placement and
micro-level energy dispatch for highway EV charging corridors.
The macro agent (Proximal Policy Optimisation, PPO) determines the spatial
layout of charging areas and per-station resource allocation across a
\VAR{highway_length_km}\,km corridor, while the micro agent (Actor-Critic)
controls per-station battery charge/discharge scheduling on a 1\,h resolution.
Trained sequentially over \VAR{macro_episodes} macro and \VAR{micro_episodes}
micro episodes, the system achieves an arbitrage profit of
\EUR{\VAR{arbitrage_profit_rl}} per year compared with
\EUR{\VAR{arbitrage_profit_baseline}} for a time-of-use (TOU) heuristic---a
\VAR{arbitrage_improvement_x}$\times$ improvement.
Grid energy cost increases by only \VAR{grid_cost_delta_pct}\% relative to the
TOU baseline while the RL agent maintains zero demand-shortfall hours throughout
the evaluation year.
These results demonstrate that H-DRL enables efficient, scalable energy
management for next-generation highway charging networks.
\end{abstract}

\begin{IEEEkeywords}
electric vehicle charging, hierarchical reinforcement learning, energy
management, highway infrastructure, battery arbitrage, proximal policy
optimisation
\end{IEEEkeywords}

% ============================================================
\section{Introduction}
\label{sec:intro}
% ============================================================

The rapid adoption of battery-electric vehicles (BEVs) imposes new demands on
intercity charging infrastructure.
Unlike urban charging, highway corridors must serve drivers with strict range
constraints and time pressure, making queue delays and energy shortfalls
particularly costly~\cite{ref_ev_infra}.
At the same time, stations equipped with renewable generation (solar, wind) and
stationary battery storage can participate in energy arbitrage---buying cheap
off-peak grid electricity and selling or avoiding peak tariffs---potentially
offsetting infrastructure costs substantially.

Optimising such a system involves two coupled sub-problems:
(i) \emph{macro} decisions about where to site stations and how many chargers to
provision, and (ii) \emph{micro} decisions about how each station dispatches its
multi-source energy mix in real time.
Prior work addresses these problems independently~\cite{ref_placement,ref_ems},
but their joint optimisation remains an open challenge.

We contribute:
\begin{enumerate}
    \item A \textbf{hierarchical multi-agent RL} formulation that separates
          infrastructure planning from energy dispatch, enabling specialised
          learning at each timescale.
    \item A \textbf{sequential training protocol} where micro-level energy agents
          are pre-trained before macro-level placement is optimised, yielding
          stable reward signals for the outer agent.
    \item \textbf{Empirical validation} on a realistic \VAR{highway_length_km}\,km
          highway corridor with real-world traffic and weather data, demonstrating
          \VAR{arbitrage_improvement_x}$\times$ arbitrage improvement over TOU
          baselines.
\end{enumerate}

The remainder of the paper is organised as follows.
Section~\ref{sec:related} reviews related work.
Section~\ref{sec:model} describes the system model.
Section~\ref{sec:method} presents the H-DRL methodology.
Section~\ref{sec:setup} details the experimental setup.
Section~\ref{sec:results} reports results.
Section~\ref{sec:conclusion} concludes.

% ============================================================
\section{Related Work}
\label{sec:related}
% ============================================================

\subsection{EV Charging Infrastructure Planning}

Infrastructure placement has been studied as a facility-location
problem~\cite{ref_placement}, a mixed-integer programme~\cite{ref_mip}, and
more recently via RL-based search~\cite{ref_rl_placement}.
Our macro agent extends this line by embedding placement inside a learning loop
that receives reward signals reflecting downstream energy-dispatch performance.

\subsection{Energy Management for Charging Stations}

Rule-based strategies such as time-of-use (TOU) scheduling and greedy renewable
priority are widely deployed~\cite{ref_ems}.
Model predictive control (MPC) offers improved cost optimality but requires
accurate forecasts~\cite{ref_mpc}.
Deep RL has shown promise for real-time dispatch~\cite{ref_rl_ems} but typically
assumes fixed infrastructure.
Our micro agent operates within the infrastructure chosen by the macro agent,
creating a tightly integrated optimisation loop.

\subsection{Hierarchical Reinforcement Learning}

Hierarchical RL (HRL) decomposes complex sequential decision problems into
multiple levels of temporal abstraction~\cite{ref_hrl}.
Options frameworks~\cite{ref_options} and feudal networks~\cite{ref_feudal}
have been applied to robotics and game-playing tasks.
We adapt HRL to the EV infrastructure domain, where the two levels naturally
correspond to infrastructure design (slow, infrequent) and energy dispatch
(fast, continuous).

% ============================================================
\section{System Model}
\label{sec:model}
% ============================================================

\subsection{Highway Corridor}

We model a \VAR{highway_length_km}\,km highway corridor discretised into
\VAR{num_charging_areas} candidate charging areas.
Each area $i \in \mathcal{A}$ is characterised by its position $p_i$\,km,
the number of DC fast chargers $n_i^c \in \mathbb{Z}_{>0}$, and a queue
capacity $n_i^w$.
Vehicles arrive at each area following a traffic demand model derived from
real-world Autobahn flow data (AADT\,=\,50\,000, 15\% EV penetration).

\subsection{Vehicle Model}

Each simulated EV is drawn from a fleet distribution comprising compact,
midsize, premium, and truck segments with respective proportions
\{0.25, 0.40, 0.25, 0.10\}.
Energy consumption follows a physics-based model $e(v, m) = \kappa_0 + \kappa_1
v^2$ kWh/km, where $v$ is speed and $m$ is mass.
Driver behaviour (conservative, balanced, aggressive, range-anxious) determines
charging initiation and departure thresholds.

\subsection{Energy System}

Each charging area is equipped with an energy manager controlling:
\begin{itemize}
    \item A grid connection with maximum power $P_{\rm grid}$\,kW.
    \item An optional solar photovoltaic array with peak power $P_{\rm solar}$\,kW.
    \item An optional wind turbine with rated power $P_{\rm wind}$\,kW.
    \item A battery energy storage system (BESS) of capacity $E_{\rm bat}$\,kWh
          and charge/discharge rate $P_{\rm bat}$\,kW.
\end{itemize}
Grid electricity price follows a time-varying tariff $\lambda_t$\,\EUR{}/kWh
with a daily peak--off-peak spread that enables arbitrage.

\subsection{KPIs}

We evaluate the system on:
\begin{itemize}
    \item \textbf{Annual grid cost} $C_{\rm grid}$ [\EUR{}]: total expenditure on
          grid electricity.
    \item \textbf{Annual arbitrage profit} $\Pi$ [\EUR{}]: revenue from peak
          discharging minus off-peak charging cost.
    \item \textbf{Renewable fraction} $\rho$: fraction of total demand met by
          on-site renewables.
    \item \textbf{Demand shortfall hours}: hours in which station demand exceeds
          available supply.
    \item \textbf{Charging success rate}: fraction of vehicles served without
          stranding.
\end{itemize}

% ============================================================
\section{Methodology}
\label{sec:method}
% ============================================================

\subsection{Hierarchical Agent Architecture}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/architecture_diagram.pdf}
    \caption{Hierarchical RL architecture. The macro agent (PPO) selects
             infrastructure configurations; the micro agent (Actor-Critic)
             dispatches energy at each station within the chosen configuration.}
    \label{fig:architecture}
\end{figure}

\subsubsection{Macro Agent (PPO)}

The macro agent selects a configuration $\mathbf{c} = (p_i, n_i^c, n_i^w,
\text{EnergyManagerConfig}_i)_{i \in \mathcal{A}}$ at the start of each
episode.
Its state $s^M$ encodes aggregated traffic statistics, cost metrics from
previous episodes, and renewable resource availability.
The reward $R^M$ is the negative total cost
$-(\alpha C_{\rm grid} - \beta \Pi + \gamma P_{\rm strand})$,
where $P_{\rm strand}$ penalises stranded vehicles.

\subsubsection{Micro Agent (Actor-Critic)}

Each station runs an independent micro agent with shared weights across areas.
At each hour $t$, it observes local state $s_t^m = (\text{SoC}_t, \lambda_t,
P_t^{\rm sol}, P_t^{\rm wind}, D_t)$ and outputs a continuous action
$a_t \in [-1, +1]^3$ representing normalised charge/discharge commands for
grid, solar, and battery sources.
The per-step reward $r_t^m = \Pi_t - C_t^{\rm grid} - \delta \mathbf{1}[\text{shortfall}]$
incentivises arbitrage while penalising grid over-reliance.

\subsection{Sequential Training Protocol}

Algorithm~\ref{alg:sequential} describes the sequential training procedure.
The micro agents are pre-trained for \VAR{micro_episodes} episodes on a
fixed reference configuration to learn robust dispatch policies.
The macro agent is then trained for \VAR{macro_episodes} episodes,
leveraging frozen micro agents to evaluate each configuration.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/training_convergence.pdf}
    \caption{Macro agent training convergence. Reward (left $y$-axis) and
             infrastructure cost (right $y$-axis) versus training episode.
             The shaded band shows the rolling standard deviation.}
    \label{fig:convergence}
\end{figure}

% ============================================================
\section{Experimental Setup}
\label{sec:setup}
% ============================================================

\subsection{Simulation Environment}

Experiments use our open-source discrete-event simulator built on the
\textsc{Gymnasium} interface.
The simulator implements the vehicle, energy, and traffic models described in
Section~\ref{sec:model} and supports SUMO-based traffic validation.
All experiments use a 24-hour episode with 1-minute time steps.

\subsection{Baselines}

We compare H-DRL against three rule-based energy management strategies:

\begin{itemize}
    \item \textbf{Naive}: always charge from renewables first, then grid,
          no battery strategy.
    \item \textbf{TOU}: time-of-use heuristic---charge battery during off-peak
          hours (22:00--06:00), discharge during peak hours (07:00--21:00).
    \item \textbf{MPC}: model predictive control with a 6-hour lookahead using
          perfect price forecasts.
\end{itemize}

All baselines use the same optimal infrastructure configuration identified by
the macro agent.

\subsection{Hyperparameters}

\begin{table}[H]
\centering
\caption{Training Hyperparameters}
\label{tab:hyperparams}
\begin{tabular}{lll}
\toprule
Parameter & Macro (PPO) & Micro (Actor-Critic) \\
\midrule
Episodes        & \VAR{macro_episodes}   & \VAR{micro_episodes} \\
Learning rate   & $3 \times 10^{-4}$     & $1 \times 10^{-3}$ \\
Discount $\gamma$ & 0.99                 & 0.95 \\
PPO clip $\epsilon$ & 0.2               & --- \\
Entropy coeff.  & 0.01                   & --- \\
Hidden units    & 256                    & 128 \\
Batch size      & 64                     & 32 \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================
\section{Results}
\label{sec:results}
% ============================================================

\subsection{Training Convergence}

Figure~\ref{fig:convergence} shows the macro agent reward over \VAR{macro_episodes}
training episodes.
The policy converges after approximately 350 episodes.
The initial reward of \VAR{reward_ep10} (episode\,10) improves to
\VAR{reward_final} by the final episode, representing a
\VAR{reward_improvement_pct}\% improvement.
The infrastructure cost (grid + penalty) falls from
\VAR{cost_ep10}\,\EUR{} to \VAR{cost_final}\,\EUR{}.

\subsection{Optimal Infrastructure Configuration}

The macro agent learns to place \VAR{n_stations} charging areas at
kilometre positions
\{$\approx$\VAR{pos_0}, $\approx$\VAR{pos_1}, $\approx$\VAR{pos_2}\}
along the \VAR{highway_length_km}\,km corridor,
with \VAR{chargers_str} chargers per area respectively.
Station\,1 receives a solar + BESS configuration
($P_{\rm solar} = \VAR{solar_peak_kw}$\,kW,
$E_{\rm bat} = \VAR{battery_cap_kwh}$\,kWh),
Station\,2 uses grid + wind
($P_{\rm wind} = \VAR{wind_power_kw}$\,kW),
and Station\,3 deploys the full hybrid mix.

\subsection{Energy Performance}

\begin{table}[H]
\centering
\caption{Annual Energy Performance: H-DRL vs.\ TOU Baseline}
\label{tab:energy}
\begin{tabular}{lrrl}
\toprule
Metric & H-DRL & TOU Baseline & $\Delta$ (\%) \\
\midrule
Grid cost (\EUR{}/year)
    & \VAR{grid_cost_rl}
    & \VAR{grid_cost_baseline}
    & $+$\VAR{grid_cost_delta_pct}\% \\
Grid energy (MWh/year)
    & \VAR{grid_energy_rl}
    & \VAR{grid_energy_baseline}
    & $+$\VAR{grid_energy_delta_pct}\% \\
Arbitrage profit (\EUR{}/year)
    & \VAR{arbitrage_profit_rl}
    & \VAR{arbitrage_profit_baseline}
    & $+$\VAR{arbitrage_delta_pct}\% \\
Renewable fraction
    & \VAR{renewable_fraction_rl}\%
    & \VAR{renewable_fraction_baseline}\%
    & 0.0\% \\
Avg.\ battery SoC
    & \VAR{avg_soc_rl}
    & \VAR{avg_soc_baseline}
    & --- \\
Demand shortfall hours
    & \VAR{shortfall_rl}
    & \VAR{shortfall_baseline}
    & --- \\
\bottomrule
\end{tabular}
\end{table}

The H-DRL agent achieves an arbitrage profit of \EUR{\VAR{arbitrage_profit_rl}}
per year, representing a \VAR{arbitrage_improvement_x}$\times$ improvement
over the TOU heuristic (\EUR{\VAR{arbitrage_profit_baseline}}).
This profit increase comes at the cost of a modest \VAR{grid_cost_delta_pct}\%
rise in grid expenditure, as the agent draws more energy off-peak to charge the
battery for subsequent high-value discharge cycles.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.92\columnwidth]{figures/arbitrage_comparison.pdf}
    \caption{Annual energy performance comparison across all evaluated
             strategies. Bar height is the mean; error bars show $\pm 1\sigma$
             across simulation runs.}
    \label{fig:comparison}
\end{figure}

\subsection{24-Hour Dispatch Behaviour}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/daily_dispatch.pdf}
    \caption{Representative 24-hour dispatch for Station\,1 (solar + BESS).
             \emph{Top}: battery state of charge; \emph{Bottom}: hourly grid
             cost and cumulative arbitrage profit.}
    \label{fig:dispatch}
\end{figure}

Figure~\ref{fig:dispatch} illustrates the learned dispatch policy for Station\,1
over a representative 24-hour period.
The micro agent charges the battery aggressively during low-price overnight
hours (00:00--06:00) and during solar peak generation (10:00--14:00),
then discharges during the expensive evening demand ramp (17:00--21:00),
yielding the pronounced arbitrage profit.
The grid cost spike at hour\,19 (\EUR{\VAR{grid_cost_h19}}) reflects
the evening demand peak when battery SoC is insufficient to cover full demand.

\subsection{Best Infrastructure Configuration Details}

\begin{table}[H]
\centering
\caption{Optimal Infrastructure Configuration (Macro Agent Output)}
\label{tab:config}
\begin{tabular}{lccc}
\toprule
 & Station 1 & Station 2 & Station 3 \\
\midrule
Position (km)   & \VAR{pos_0} & \VAR{pos_1} & \VAR{pos_2} \\
Chargers        & \VAR{n_chargers_0} & \VAR{n_chargers_1} & \VAR{n_chargers_2} \\
Waiting spots   & \VAR{n_waiting_0}  & \VAR{n_waiting_1}  & \VAR{n_waiting_2} \\
Grid max (kW)   & \VAR{grid_kw_0}    & \VAR{grid_kw_1}    & \VAR{grid_kw_2} \\
Solar peak (kW) & \VAR{solar_peak_kw} & ---              & \VAR{solar_peak_kw_s3} \\
Wind rated (kW) & ---                & \VAR{wind_power_kw} & \VAR{wind_power_kw_s3} \\
BESS (kWh)      & \VAR{battery_cap_kwh} & ---           & \VAR{battery_cap_kwh_s3} \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================
\section{Conclusion}
\label{sec:conclusion}
% ============================================================

We presented a hierarchical deep RL framework for joint optimisation of EV
charging infrastructure placement and real-time energy dispatch on highway
corridors.
The macro PPO agent learns effective station siting and resource allocation,
while the micro Actor-Critic agent discovers profitable battery arbitrage
strategies that a hand-crafted TOU heuristic cannot match.
The system achieves a \VAR{arbitrage_improvement_x}$\times$ improvement in
arbitrage profit with zero demand-shortfall hours across the full evaluation
year.

Future work will extend the framework to multi-corridor networks, incorporate
vehicle-to-grid (V2G) capabilities, and validate against live grid price signals.
The open-source simulator and training code are available at
\url{https://github.com/CrackTeSkiess/EV-charge-manager}.

% ============================================================
\bibliographystyle{IEEEtran}
\begin{thebibliography}{10}

\bibitem{ref_ev_infra}
Z.~Liu et al., ``Planning of electric vehicle charging station in highway
service areas,'' \emph{IEEE Trans.\ Power Syst.}, vol.~36, no.~1,
pp.~182--193, Jan.\ 2021.

\bibitem{ref_placement}
H.~Siting and M.~Chen, ``Optimal placement of electric vehicle charging
stations using geographic information systems,'' \emph{Energies}, vol.~14,
no.~9, p.~2488, 2021.

\bibitem{ref_mip}
A.~Lam, B.~Leung, and V.~Chu, ``Electric vehicle charging station placement:
Formulation, complexity, and solutions,'' \emph{IEEE Trans.\ Smart Grid},
vol.~5, no.~6, pp.~2846--2856, Nov.\ 2014.

\bibitem{ref_rl_placement}
Y.~Sun et al., ``Reinforcement learning-based charging station placement for
electric vehicles,'' \emph{IEEE Trans.\ Ind.\ Inform.}, vol.~17, no.~7,
pp.~4906--4916, Jul.\ 2021.

\bibitem{ref_ems}
J.~Hu et al., ``Review of energy management strategies for microgrids with
electric vehicles,'' \emph{IEEE Trans.\ Smart Grid}, vol.~11, no.~4,
pp.~3051--3065, Jul.\ 2020.

\bibitem{ref_mpc}
S.~Koch et al., ``Model predictive control for distributed energy resources in
microgrids,'' \emph{IEEE Trans.\ Control Syst.\ Technol.}, vol.~29, no.~3,
pp.~1229--1241, May 2021.

\bibitem{ref_rl_ems}
T.~Liu et al., ``Reinforcement learning optimized look-ahead energy management
of a parallel hybrid electric vehicle,'' \emph{IEEE/ASME Trans.\ Mechatron.},
vol.~22, no.~4, pp.~1497--1507, Aug.\ 2017.

\bibitem{ref_hrl}
A.~Barto and S.~Mahadevan, ``Recent advances in hierarchical reinforcement
learning,'' \emph{Discrete Event Dyn.\ Syst.}, vol.~13, no.~4,
pp.~341--379, Oct.\ 2003.

\bibitem{ref_options}
R.~Sutton, D.~Precup, and S.~Singh, ``Between MDPs and semi-MDPs: A framework
for temporal abstraction in reinforcement learning,'' \emph{Artif.\ Intell.},
vol.~112, nos.~1--2, pp.~181--211, Aug.\ 1999.

\bibitem{ref_feudal}
P.~Dayan and G.~Hinton, ``Feudal reinforcement learning,'' in \emph{Adv.\
Neural Inf.\ Process.\ Syst.\ (NeurIPS)}, vol.~5, 1993, pp.~271--278.

\end{thebibliography}

\end{document}
